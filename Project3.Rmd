---
title: "Project3"
author: "Ruben Sowah and  Zhiyuan Yang"
date: "2022-10-30"
output: rmarkdown::github_document
params: 
  chan: 'data_channel_is_entertainment'
    
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message  = F, warning = F, cache = T)
```


```{r, echo = F, eval = F}
#rmarkdown::render("project3.Rmd",
                  #output_format = "github_document",
                  #output_file = "README.md",
                  #output_options = list(
                  #html_preview = FALSE, toc = TRUE, toc_depth = 2, toc_float = TRUE))
```
# Introduction

The dataset that we used is called Online News Popularity Data set. This dataset concluded multiple features of articles published by Mashable in past years. Our goal is to use different predictive models to predict the number of shares in social networks. Our target variable is the number of shares. The variable name of our target variable is called shares. Thus, shares will be our dependent variable in our predictive models. After our discussion, we both think the rate of unique words in the content, the number of links, the number of images, the number of videos, whether the article was published on the weekend, the rate of positive words in the content, the rate of negative words in the content, the average polarity of positive words, the average polarity of negative words, whether the article published on Monday or on a Saturday will affect the number of shares for article. Thus, we selected ** n_tokens_content, num_hrefs, num_imgs, num_videos, is_weekend, global_rate_positive_words, global_rate_negative_words, avg_positive_polarity, avg_negative_polarity, weekday_is_monday, weekday_is_saturday ** as our independent variables.

This dataset has six different channels, which are a lifestyle channel, an entertainment channel, a bus channel, a social media channel, a tech channel, and a world channel. We will subset the dataset based on different channel types before we create our predictive models. 

We will fit a random forest model and fit a boosted tree model. Both models will be chosen using cross-validation. We will describe those in more detail later.

# Load packages
```{r}
library(tidyverse)
library(caret)
library(Metrics)
library(ggplot2)
library(readr)
library(corrplot)
library(knitr)
library(rsample)
library(randomForest)
library(rmarkdown)
library(tibble)
library(haven)
```

# Read in the data

```{r}
## Read and get an overview of the data
newsdata <- read_csv("OnlineNewsPopularity.csv")
#newsdata <- read_csv("C:\\Users\\zyang\\Desktop\\OnlineNewsPopularity\\OnlineNewsPopularity.csv")
head(newsdata)

## Subset the data by the channels, and select our desired features
newsdata <- newsdata %>% 
        #filter(params$chan == 1) %>%
        filter(!!rlang::sym(params$chan) == 1) %>%
       # filter(data_channel_is_entertainment == 1 ,data_channel_is_world == 1) %>%
        select(n_tokens_content,num_hrefs,num_imgs, num_videos,weekday_is_monday,weekday_is_saturday,is_weekend,global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,avg_negative_polarity,shares)#,data_channel_is_entertainment,data_channel_is_world) 
        
## Coerce the categorical variables into factor
newsdata$weekday_is_monday <- factor(newsdata$weekday_is_monday, levels = c(0,1), labels = c('Not Monday', 'It is Monday'))

newsdata$weekday_is_saturday <- factor(newsdata$weekday_is_saturday, levels = c(0,1), labels = c('Not Saturday', 'It is Saturday'))

newsdata$is_weekend <- factor(newsdata$is_weekend, levels = c(0,1), labels = c('Not a weekend', 'Weekend'))

## View data
print(newsdata, width = 100, n = 10)
```

# First group member's summarizations

<br>

#### [1) Numerical summaries]{.underline}  

  Here I will get some numerical summaries like the mean, the standard deviation , the variance of some of the quantitative variables as well as get the count of the categorical variables.
  
```{r}
## Get the numerical summaries of some numeric features
num.summary <- newsdata %>%
          summarize(tokens.avg = mean(n_tokens_content), image.avg = mean(num_imgs), vids.avg = mean(num_videos), pos.words.dev = sd(global_rate_positive_words), links.var = var(num_hrefs))

num.summary

## Get contingency tables of the categorical features

# Count of the articles published and not on Monday
table(newsdata$weekday_is_monday)

# Two ways table of articles published on weekend and on Saturday or neither.
table(newsdata$is_weekend, newsdata$weekday_is_saturday)
```

  * From the numerical summaries, the results show that there is an average of 607 words in the content, an average of 6 images and 3 videos. The standard deviation of the positive words from the mean is 0.0169 and the number of links varies by a average amount of 167.  
  
  * The one way contingency table tells us that the number of articles published on Monday is less compared to the number of articles that is not published on Monday, 1358 versus 5699.  
  
  * From the two ways contingency tables, 536 articles are published during the weekend, but it is not on Saturday. The amount of articles published on Saturday is 380. A total number of 6141 articles are not published during the weekend.
  
  
#### [2) Graphs]{.underline}

 * __Scatter plot of the rate of positive words in the content and the number of shares__
```{r}
g <- ggplot(newsdata, aes(x = global_rate_positive_words, y = shares))
g + geom_point(color = 'blue')+
  labs(title = 'Rate of positive words vs Number of shares')
```

  A scatter plot is used to visualize the relation between two numeric variables. A strong positive relationship between the rate of positive words and the number of shares will show a linear upward trend with the data points closed to each other. This means that the number of shares grows as the number of positive words increases. 
  
  A negative relationship between the two variables is shown by a downward trend that tells us that people share less contents that have lots of positive words.  
  
 * __Density plot__
```{r}
g <- ggplot(newsdata, aes(x = global_rate_negative_words)) 
g + geom_density(kernel ='gaussian', color = 'red', size = 2)+
  labs(title = 'Density plot  of the rate of negative words in the article')
```
 
  A density plot can tell us about the distribution of a certain feature or the whole data. Here, we plot the density of the rate of negative words. A right skewed plot is an indication that there are quite more negative words in the article. A left skewed plot indicates that there are not much of negative words in the article. A symmetric plot tells us that the amount of negative words in the article is normally distributed, about average.  
  
<br>

  * __Dotplot__
```{r}
g <- ggplot(newsdata, aes(x = is_weekend, y = shares)) 
g + geom_dotplot(binaxis = "y", stackdir = 'center', color = 'magenta', dotsize = 1.2)+
  labs(title = 'Dotplot of the number of articles shared vs the week of the day')
```

  * Similarly to a boxplot, dotplots can be used to visualize the five number summary of a numeric data. Here , we are trying to see graphically the number of contents shared during the weekday and the weekend. We would expect the minimum number to be 0, since a the least amount of contents that can be shared can't go below 0.  
  
  * A greater number of points, for example in the 'Not weekend' group states that more articles are shared during  the week days compared the weekend. The opposite would mean that contents are shared more during the weekend.
  
  * Points that are far away from the rest indicates possible outliers.
  
  
# First group member's modeling
   
  Here the data will be split into two, a training set and a testing set. Two different models will be fit on the training set , then later be evaluated on the test set. The two models that will be fit are a **linear regression model** and a **random forest model**, using cross-validation.  
  
  * __What is linear regression about ?__ 
  
  Linear regression (LR) is the simplest form of a supervised machine learning, where the data has both a single (simple linear regression) or numerous predictors variables (multiple linear regression) denoted X's and an outcome or response variable denoted Y, that is quantitative. Linear regression is used for either predicting the response variable or to understand the relationship between the response and the predictors. In the former case, we talk about prediction and in the latter, we talk about inference.  
    
  Though a very simple approach , LR is widely used in practice and lots of advanced models are a generalization of LR. With LR, one can seek to understand if there is a relation between the response and the predictors, and how strong that relationship is. Which predictors are associated with the response, how accurately can one predicts the response, is the relationship linear or non-linear, are the predictors correlated? Those are some important questions one can answers with the use of linear regression.  
    
    
  * __What is random forest about ?__
  
  Random forest (RF) is supervised statistical machine learning algorithm , constructed from decision trees, that is used in regression and classification problems. RF is part of a general learning method called *ensemble learning*. The idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models, or in layman terms, an ensemble learning simply means combining multiple models. 
  
   RF builds decision trees on different samples and takes their majority vote for classification and average for regression. It is an extension of another ensemble learning method called *Bagging or Bootstrap Aggregation*. Bagging chooses a random sample from the data, and generates different models from those samples called Bootstrap samples, the sample is usually done with replacement. 
   
   Rf is an extension of Bagging in the sense that RF doesn't use all the predictors unlike Bagging. It uses a random subset of predictors for each bootstrap sample, and the final output is based on the average or majority  ranking, in this way the problem of overfitting is also avoided.  
  
  
#### [__1) Fit a linear regression model__]{.underline}
  
  The data now will be split into a train and test sets, and a multiple linear regression model will be fit on the train set. The train set will be 70 percent of the whole data and the remaining 30 % will be the test set.  
```{r}
## Set a seed for reproducible random numbers
set.seed(12)

## Using the rsample package, create a training an test set (70/30)
index <- initial_split(newsdata, prop = 0.7)
train.set <- training(index)
test.set <- testing(index)

## Fit a linear regression model
regmod <- train(shares ~. ,
                data = train.set,
                method = 'lm',
                preProcess = c('center','scale'),
                trControl = trainControl(method = 'cv', number = 5)
                )
summary(regmod)
```


#### [__2) Fit a random forest model__]{.underline} 

  Here a random forest model will be fit on the train set using a cross-validation with 5 folds. We will use the expand.grid() function to select a range of parameters that will be tuned in our model. The optimal parameter that minimizes th error will be chosen and the model will be refit on the train set using that optimal parameter. We will also center and scale the train data for a more accurate distribution of the variables.  
```{r, eval = F}
## Create a grid of tuning parameters
forestgrid <- expand.grid(mtry = c(1:20))

## Fit the random forest model
forestmod <- train(shares ~ . ,
                   data = train.set,
                   method = 'rf',
                   trControl = trainControl(method = 'cv', number= 5),
                   preProcess = c('center','scale'),
                   tuneGrid = forestgrid)
forestmod

## Get the optimal tuned parameter
mtry.opt <- forestmod$bestTune$mtry

## Refit the random forest model on the train set using the optimal tuned parameter
forest.tuned <-  train(shares ~ . ,
                   data = train.set,
                   method = 'rf',
                   trControl = trainControl(method = 'cv', number= 5),
                   preProcess = c('center','scale'),
                   tuneGrid = expand.grid(mtry = mtry.opt))


```


# Second group member's summarizations
```{r}
# Create contingency table of whether the article was published on the weekend
table(newsdata$is_weekend)
```
***Comments:***
Based on the contingency table, we can see how many articles are published on weekend.
0 means articles are not published on weekend. 1 means articles are published on weekend.

```{r}
# Create contingency table of whether the article was published on the Saturday
table(newsdata$weekday_is_saturday)
```

***Comments:***
Based on the contingency table, we can see how many articles are published on Saturday.
0 means articles are not published on Saturday. 1 means articles are published on Saturday.

```{r}
# Create bar plot to see whether the article was published on the Saturday

ggplot(newsdata, aes(x=weekday_is_saturday))+
  geom_bar(aes(fill = "drv")) + 
  labs(y="Number of the Articles Were Published on the Saturday", 
       title= "Weekend published article's Bar plot")
```
***Comments:***
Based on the bar plot, we can see how many articles are published on Saturday.



```{r}
# Create histogram to see number of shares and whether the article was published on the Weekend

ggplot(data = newsdata, aes(x = shares))+ 
  geom_histogram(bins = 20, aes(fill = is_weekend)) +
  labs(x = "Number of Shares",
       y="Number of the Articles Were Published on the Weekend", 
       title = "Histogram of Shares that are Related to Weekend") +
       scale_fill_discrete(name = "Whether Weekend Published", 
                           labels = c("No", "Yes"))

```

***Comments:***
Based on this histogram, we can see the distribution of the number of shares. 
If the peak of the graph lies to the left side of the center, it means that 
most of articles have small number of shares. 
If the peak of the graph lies to the right side of the center, it means that most of articles have large number of shares. 
If we see a bell shape, it means that the number of articles have large number of shares is similar with 
the number of articles have small number of shares. 
The No means the articles were published on weekend. The Yes means
the articles were published on weekend.

```{r}
g <- ggplot(newsdata, aes(x = n_tokens_content, y = shares))
g + geom_point(color = 'green')+
  labs(title = 'number of tokens content vs Number of shares')

```

***Comments:***
Based on this scatter plot, we can see how many points plotted in the Cartesian plane. Each point represents the values of number of shares and number of token content. The closer the data points come to forming a straight line when plotted, it means that number of shares and number of token content have stronger the relationship. If the data points make a straight line going from near the origin out to high y-values, variables will have a positive correlation.



# Second group member's modeling

#### [Fit another linear regression model]{.underline}
```{r}
regmod2 <- train(shares~(n_tokens_content+num_hrefs+num_imgs+num_videos+global_rate_positive_words+
                           global_rate_negative_words+
                           avg_positive_polarity+avg_negative_polarity)^2+
                           weekday_is_monday+weekday_is_saturday+
                          is_weekend,
                 data = train.set,
                 method = "lm",
                 preProcess = c("center", "scale"),
                 trControl = trainControl(method = "cv", number = 10))

summary(regmod2)
```

#### What is a boosted tree model?

The boosted tree model is a general approach that can be applied to trees.
Trees grown sequentially and each subsequent tree is grown on a modified version of original data.
When tree growing, the predictions also are updated.
Thus, it solves errors that created by previous decision trees. 
Boosting transforms weak decision trees, which are weak learners into strong learners.
Boosting is an iterative process. Each tree is dependent on the previous tree. 
For the procedure, we can initialize predictions as 0, and Find the residuals (observed-predicted), call the set of them r.
And then we fit a tree with splits (terminal nodes) treating the residuals as the
response, which they are for the first fit. After that, we can update predictions and update 
residuals for new predictions and repeat B times. 


#### [Fit a boosted tree model]{.underline} 

```{r}
boosted_fit <- train(shares ~., data = train.set, method = "gbm",
                       trControl = trainControl(method = "repeatedcv", 
                                                number = 5, repeats = 3),
                       preProcess = c("center", "scale"),
                       tuneGrid = expand.grid(n.trees = c(25, 50, 100, 150, 200, 250),
                                              interaction.depth = 1:5,
                                              shrinkage = 0.1,
                                              n.minobsinnode = 10),
                       verbose = FALSE)
boosted_fit

```


# Comparison of the four models  

  We will predict the four models fitted above on the test set and use the postResample() function to get the test metrics. We are more concerned about the root mean squared error (RMSE) as the measure of our models.
  
```{r,eval = F}
## Predict the multiple regression fit on the test set
regmod.pred <- predict(regmod, newdata = test.set)

## Get the RMSE of the regression model
regmod.rmse <- postResample(regmod.pred, test.set$shares)[1]

## Predict the RF model on the test set 
forest.pred <- predict(forest.tuned, newdata = test.set)

## Get the RMSE of the Random Forest model
forest.rmse <- postResample(forest.pred, test.set$shares)[1]

## Predict the second linear regression model on the test set
regmod2.pred <- predict(regmod2, newdata = test.set)

## Get the RMSE of the second linear regression model
regmod2.rmse <- postResample(regmod2.pred, test.set$shares)[1]

## Predict the boosted tree model on the test set
boosted.pred <- predict(boosted_fit, newdata = test.set)

## Get the RMSE of the boosted tree model
boosted.rmse <- postResample(boosted.pred, test.set$shares)[1]

## Combine the four RMSE in a table
data.frame(Regression = regmod.rmse,
           Forest = forest.rmse,
           Pol.regression = regmod2.rmse,
           Boosted.tree = boosted.rmse)
```

  The **random forest** model has the lowest root mean squared error of all four models, with a value of **7951.099**, hence is our winner model. 


# Automation

```{r, echo = T , eval = F}
channels <- c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus", "data_channel_is_socmed", "data_channel_is_tech", "data_channel_is_world")

## Files
output_file <- paste0(channels,".md")

## Create a list for each channel with just channel name parameter
 params = lapply(channels, FUN = function(x){
 
   return(list(chan = x))

})
## Put into a data frame
reports = tibble(channels, output_file, params);reports

## Automation

apply(reports, MARGIN = 1, FUN = function(x){

  rmarkdown::render(input = "Project3.Rmd",

                    output_format = "github_document",

                    output_file = x[[2]],

                    params = x[[3]],

                    output_options = list(html_preview = FALSE))

})
```



```{r render, echo=TRUE, eval=F}
rmarkdown::render(input = "Project3.Rmd", 
                  output_file = x[[2]], 
                  output_format = "github_document",
                  params = list(Channels = 'data_channel_is_entertainment'))
```


